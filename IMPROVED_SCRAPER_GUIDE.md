# ğŸš€ GeliÅŸmiÅŸ Scraper KullanÄ±m KÄ±lavuzu

## ğŸ“‹ Ä°Ã§indekiler
1. [Yeni Ã–zellikler](#yeni-Ã¶zellikler)
2. [Kurulum](#kurulum)
3. [KullanÄ±m](#kullanÄ±m)
4. [Hata YÃ¶netimi](#hata-yÃ¶netimi)
5. [Ä°zleme ve Raporlama](#izleme-ve-raporlama)
6. [Sorun Giderme](#sorun-giderme)
7. [En Ä°yi Uygulamalar](#en-iyi-uygulamalar)

## âœ¨ Yeni Ã–zellikler

### ğŸ”„ Retry MekanizmasÄ±
- **Otomatik Yeniden Deneme**: BaÅŸarÄ±sÄ±z scraping iÅŸlemleri otomatik olarak 3 kez denenir
- **Exponential Backoff**: Her denemede bekleme sÃ¼resi katlanarak artar
- **AkÄ±llÄ± BaÅŸarÄ± KontrolÃ¼**: Sadece gerÃ§ekten baÅŸarÄ±lÄ± sonuÃ§lar kabul edilir

### â±ï¸ Rate Limiting
- **Domain BazlÄ± SÄ±nÄ±rlama**: Her domain iÃ§in saniyede maksimum 2 istek
- **Otomatik Bekleme**: Rate limit aÅŸÄ±ldÄ±ÄŸÄ±nda otomatik bekleme
- **AkÄ±llÄ± Zamanlama**: 60 saniyelik pencere iÃ§inde istekleri yÃ¶netir

### ğŸ›¡ï¸ GeliÅŸmiÅŸ Bot KorumasÄ± AÅŸma
- **Stealth Scripts**: GeliÅŸmiÅŸ bot tespiti engelleme
- **Mobil User-Agent**: iPhone Safari simÃ¼lasyonu
- **Fingerprinting Engelleme**: Canvas, WebGL, Audio fingerprinting korumasÄ±
- **Site-Specific YaklaÅŸÄ±mlar**: Her site iÃ§in Ã¶zel stratejiler

### ğŸ“Š Ä°zleme ve Raporlama
- **GerÃ§ek ZamanlÄ± Ä°statistikler**: BaÅŸarÄ± oranlarÄ±, hata analizi
- **Domain BazlÄ± Analiz**: Hangi sitelerde sorun yaÅŸandÄ±ÄŸÄ±nÄ± tespit
- **Hata Kategorileri**: Timeout, bot detection, selector not found vb.
- **Otomatik Ã–neriler**: Hata tÃ¼rÃ¼ne gÃ¶re dÃ¼zeltme Ã¶nerileri

### ğŸ”§ GeliÅŸmiÅŸ Hata Yakalama
- **DetaylÄ± Loglama**: TÃ¼m hatalar timestamp ile kaydedilir
- **Hata Kategorileri**: Hatalar tÃ¼rlerine gÃ¶re sÄ±nÄ±flandÄ±rÄ±lÄ±r
- **Otomatik Analiz**: Hata paternleri otomatik tespit edilir

## ğŸ› ï¸ Kurulum

### Gereksinimler
```bash
pip install playwright flask flask-login
playwright install chromium
```

### KonfigÃ¼rasyon
```python
# app.py iÃ§inde otomatik olarak yapÄ±landÄ±rÄ±lÄ±r
# Ek konfigÃ¼rasyon gerekmez
```

## ğŸ“– KullanÄ±m

### Temel KullanÄ±m
```python
import asyncio
from app import scrape_product

# Async scraping
async def main():
    url = "https://www.hepsiburada.com/product/123"
    result = await scrape_product(url)
    print(result)

# Ã‡alÄ±ÅŸtÄ±r
asyncio.run(main())
```

### Flask Route ile KullanÄ±m
```python
@app.route("/add_product", methods=["POST"])
@login_required
def add_product():
    url = request.form.get('url')
    result = asyncio.run(scrape_product(url))
    # Sonucu iÅŸle...
```

## ğŸ” Hata YÃ¶netimi

### Hata TÃ¼rleri ve Ã‡Ã¶zÃ¼mler

#### 1. Timeout HatalarÄ±
**Belirtiler:**
- Sayfa yÃ¼klenme sÃ¼resi aÅŸÄ±mÄ±
- Network baÄŸlantÄ± sorunlarÄ±

**Ã‡Ã¶zÃ¼mler:**
```python
# Timeout sÃ¼resini artÄ±r
await page.goto(url, timeout=60000)  # 60 saniye
await page.wait_for_timeout(5000)    # 5 saniye ek bekleme
```

#### 2. Bot Detection HatalarÄ±
**Belirtiler:**
- "Access Denied" mesajlarÄ±
- CAPTCHA sayfalarÄ±
- "BIR DAKIKA" uyarÄ±larÄ±

**Ã‡Ã¶zÃ¼mler:**
```python
# Stealth script'leri gÃ¼ncelle
await page.add_init_script(get_advanced_stealth_script())

# User-Agent deÄŸiÅŸtir
context = await browser.new_context(
    user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15"
)
```

#### 3. Selector Not Found HatalarÄ±
**Belirtiler:**
- CSS selector'lar bulunamÄ±yor
- Sayfa yapÄ±sÄ± deÄŸiÅŸmiÅŸ

**Ã‡Ã¶zÃ¼mler:**
```python
# GeliÅŸmiÅŸ selector'larÄ± kullan
enhanced_selectors = get_enhanced_selectors()
selectors = enhanced_selectors.get(domain, default_selectors)
```

### Hata Ä°zleme
```python
from app import get_scraping_stats, analyze_and_suggest_fixes

# Ä°statistikleri al
stats = get_scraping_stats()
print(f"BaÅŸarÄ± oranÄ±: {stats['success_rate']:.1f}%")

# Hata analizi
analysis = analyze_and_suggest_fixes()
print(f"Ã–neriler: {analysis['suggestions']}")
```

## ğŸ“Š Ä°zleme ve Raporlama

### API Endpoint'leri

#### 1. Scraping Ä°statistikleri
```bash
GET /api/scraping/stats
```
**YanÄ±t:**
```json
{
  "total_requests": 100,
  "successful_requests": 85,
  "failed_requests": 15,
  "success_rate": 85.0,
  "domain_stats": {
    "hepsiburada.com": {"success": 20, "failed": 2},
    "mango.com": {"success": 15, "failed": 8}
  }
}
```

#### 2. SaÄŸlÄ±k Durumu
```bash
GET /api/scraping/health
```
**YanÄ±t:**
```json
{
  "status": "healthy",
  "success_rate": 85.0,
  "total_requests": 100,
  "recent_errors_count": 3
}
```

#### 3. Son Hatalar
```bash
GET /api/scraping/errors
```
**YanÄ±t:**
```json
{
  "errors": [
    {
      "timestamp": "2024-01-15T10:30:00",
      "url": "https://example.com",
      "domain": "example.com",
      "error": "Timeout error",
      "attempt": 2
    }
  ],
  "total_errors": 15
}
```

#### 4. Ä°yileÅŸtirme Ã–nerileri
```bash
GET /api/scraping/suggestions
```
**YanÄ±t:**
```json
{
  "error_analysis": {
    "timeout": 5,
    "bot_detection": 3
  },
  "suggestions": [
    "Sayfa yÃ¼kleme sÃ¼resini artÄ±rÄ±n",
    "Daha uzun bekleme sÃ¼releri ekleyin"
  ]
}
```

### Test RaporlarÄ±
```bash
python test_improved_scraper.py
```

**OluÅŸturulan Dosyalar:**
- `test_report.json`: DetaylÄ± test raporu
- `scraping.log`: Scraping loglarÄ±

## ğŸ”§ Sorun Giderme

### YaygÄ±n Sorunlar ve Ã‡Ã¶zÃ¼mleri

#### 1. DÃ¼ÅŸÃ¼k BaÅŸarÄ± OranÄ±
**Belirtiler:** %60'Ä±n altÄ±nda baÅŸarÄ± oranÄ±

**Ã‡Ã¶zÃ¼mler:**
```python
# Rate limiting ayarlarÄ±nÄ± gÃ¼ncelle
RATE_LIMIT_PER_DOMAIN = 1  # Daha az istek
RATE_LIMIT_WINDOW = 120    # Daha uzun pencere

# Retry sayÄ±sÄ±nÄ± artÄ±r
result = await retry_scraping(url, max_retries=5, base_delay=3)
```

#### 2. Belirli Site'lerde Sorun
**Belirtiler:** Belirli domain'lerde sÃ¼rekli hata

**Ã‡Ã¶zÃ¼mler:**
```python
# Site-specific konfigÃ¼rasyon ekle
SITE_CONFIGS["problematic-site.com"] = {
    "wait_time": 10000,  # 10 saniye bekle
    "timeout": 90000,    # 90 saniye timeout
    "retry_count": 5     # 5 kez dene
}
```

#### 3. Memory SorunlarÄ±
**Belirtiler:** YÃ¼ksek memory kullanÄ±mÄ±

**Ã‡Ã¶zÃ¼mler:**
```python
# Browser'Ä± dÃ¼zenli olarak kapat
await browser.close()

# Cache'i temizle
scraping_cache.clear()
```

### Debug Modu
```python
# Debug loglarÄ±nÄ± aktifleÅŸtir
logging.basicConfig(level=logging.DEBUG)

# DetaylÄ± hata bilgisi
try:
    result = await scrape_product(url)
except Exception as e:
    print(f"DetaylÄ± hata: {e}")
    import traceback
    traceback.print_exc()
```

## ğŸ¯ En Ä°yi Uygulamalar

### 1. Rate Limiting
```python
# Her domain iÃ§in uygun rate limit ayarla
RATE_LIMIT_PER_DOMAIN = {
    "hepsiburada.com": 2,
    "mango.com": 1,      # Bot korumasÄ± yÃ¼ksek
    "sahibinden.com": 1  # Bot korumasÄ± yÃ¼ksek
}
```

### 2. Error Handling
```python
# HatalarÄ± yakala ve logla
try:
    result = await scrape_product(url)
    if not result or result.get('name') == "Ä°sim bulunamadÄ±":
        # Alternatif yÃ¶ntem dene
        result = await alternative_scraping_method(url)
except Exception as e:
    log_scraping_error(url, e)
    # Fallback sonuÃ§ dÃ¶ndÃ¼r
    return get_fallback_result(url)
```

### 3. Monitoring
```python
# DÃ¼zenli saÄŸlÄ±k kontrolÃ¼
def monitor_scraping_health():
    stats = get_scraping_stats()
    if stats['success_rate'] < 70:
        # UyarÄ± gÃ¶nder
        send_alert(f"DÃ¼ÅŸÃ¼k baÅŸarÄ± oranÄ±: {stats['success_rate']}%")
    
    # Problemli domain'leri raporla
    problematic_domains = get_problematic_domains(stats)
    if problematic_domains:
        send_report(problematic_domains)
```

### 4. Cache YÃ¶netimi
```python
# Cache sÃ¼resini ayarla
CACHE_DURATION = 3600  # 1 saat

# Cache'i dÃ¼zenli temizle
def cleanup_cache():
    current_time = time.time()
    expired_keys = [
        key for key, (data, timestamp) in scraping_cache.items()
        if current_time - timestamp > CACHE_DURATION
    ]
    for key in expired_keys:
        del scraping_cache[key]
```

### 5. Site-Specific Optimizasyonlar
```python
# Her site iÃ§in Ã¶zel ayarlar
SITE_SPECIFIC_SETTINGS = {
    "mango.com": {
        "pre_navigation": True,  # Ana sayfaya Ã¶nce git
        "cookie_handling": True, # Cookie banner'Ä± kabul et
        "human_behavior": True,  # Ä°nsan benzeri davranÄ±ÅŸ
        "wait_time": 5000        # 5 saniye bekle
    },
    "sahibinden.com": {
        "bot_protection_check": True,  # Bot korumasÄ± kontrolÃ¼
        "reload_on_error": True,       # Hata durumunda yeniden yÃ¼kle
        "wait_time": 3000              # 3 saniye bekle
    }
}
```

## ğŸ“ˆ Performans Ä°yileÅŸtirme

### 1. Parallel Processing
```python
# Birden fazla URL'yi paralel iÅŸle
async def scrape_multiple_urls(urls):
    tasks = [scrape_product(url) for url in urls]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results
```

### 2. Connection Pooling
```python
# Browser context'lerini yeniden kullan
browser_contexts = {}

async def get_or_create_context(domain):
    if domain not in browser_contexts:
        browser_contexts[domain] = await browser.new_context()
    return browser_contexts[domain]
```

### 3. Smart Caching
```python
# AkÄ±llÄ± cache stratejisi
def should_cache_result(result):
    return (
        result and 
        result.get('name') and 
        result.get('price') and
        result.get('name') != "Ä°sim bulunamadÄ±"
    )
```

## ğŸš¨ Acil Durumlar

### 1. TÃ¼m Scraping BaÅŸarÄ±sÄ±z
```python
# Emergency mode
EMERGENCY_MODE = True

if EMERGENCY_MODE:
    # Daha uzun bekleme sÃ¼releri
    RATE_LIMIT_PER_DOMAIN = 1
    RATE_LIMIT_WINDOW = 300  # 5 dakika
    
    # Daha fazla retry
    MAX_RETRIES = 5
    BASE_DELAY = 5
```

### 2. Belirli Site Tamamen Ã‡alÄ±ÅŸmÄ±yor
```python
# Site'yi geÃ§ici olarak devre dÄ±ÅŸÄ± bÄ±rak
DISABLED_SITES = ["problematic-site.com"]

def is_site_disabled(url):
    domain = extract_domain_from_url(url)
    return domain in DISABLED_SITES
```

### 3. Memory Overflow
```python
# Memory temizleme
import gc

def emergency_cleanup():
    # Cache'i temizle
    scraping_cache.clear()
    
    # Browser'larÄ± kapat
    for context in browser_contexts.values():
        await context.close()
    browser_contexts.clear()
    
    # Garbage collection
    gc.collect()
```

## ğŸ“ Destek

### Log DosyalarÄ±
- `scraping.log`: Ana scraping loglarÄ±
- `test_report.json`: Test sonuÃ§larÄ±
- `error_log.json`: Hata detaylarÄ±

### Debug KomutlarÄ±
```bash
# Test Ã§alÄ±ÅŸtÄ±r
python test_improved_scraper.py

# LoglarÄ± kontrol et
tail -f scraping.log

# Ä°statistikleri gÃ¶rÃ¼ntÃ¼le
curl http://localhost:5000/api/scraping/stats
```

### Ä°letiÅŸim
Sorun yaÅŸadÄ±ÄŸÄ±nÄ±zda ÅŸu bilgileri paylaÅŸÄ±n:
1. Hata mesajÄ±
2. URL
3. Scraping loglarÄ±
4. Test raporu
5. Sistem bilgileri

---

**Not:** Bu kÄ±lavuz sÃ¼rekli gÃ¼ncellenmektedir. Yeni Ã¶zellikler ve iyileÅŸtirmeler iÃ§in dÃ¼zenli olarak kontrol edin.
